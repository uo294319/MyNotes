# Deep Learning & CNN
---
[Go Back](UNIOVI/3S2_IntSys/README.md)

---
## Deep Learning 
### Objective
- Given a dataset $D = {(x, y)  : x \leadsto y}$
- Design the non-linear function $f : R^m \to R^n, f(x, \theta) = f_\theta(x)$
- Find $\theta$ so that minimises the mean of the loss function
	- $\theta \leftarrow \frac{1}{|D|}\sum_{(x, y)\in D}loss(y, f_\theta(x))$
- How to achieve non linearity?
	- Activation functions
		- ReLu. $g(z) = \max(0, z)$ (most used nowadays)
		- Hyperbolic tangent. (-1 and 1)
		- Sigmoid. (0 and 1) (used for classification problems)
	- Several nested layers:
		- Hidden layer 1. $h_1 = g(W_1x+b_1)$
		- Hidden layer 2. $h_2 = g(W_2h_1+b_2)$
		- Output layer. $g(x) =  g(W_2\times g(W_1x+b_1)+b_2)$
	- Optimisation needs derivatives. Gradient Descent.
- Universal Approximator Theorem
	- One hidden layer is enough to represent an approximation of any function.
	- But choose wider networks because:
		- It avoids exponential # neurons needed.
		- Better generalisation and avoids overfitting.
- Types
	- Feedforward or Multi-Layer Perceptrons (MLP) are fully connected.
	- Convolutional Neural Networks (CNN). For processing data in grid-like topology.
	- Recurrent Neural Networks (RNN). For processing sequential data. Extend MLP.
- Learning Tasks
	- Regression.
		- $Pr(y|x, \theta) = N( y, f(x, \theta), \sigma^2)$.
		- Probability distribution involving $f$
		- Minimize MSE $\arg\min_\theta \sum_{D}(y - f_\theta(x, \theta))^2$
	- Binary Classification
		- $Pr(y|x, \theta) = \sigma((2y-1)f(x,\theta))$.
		- Not a probability distribution.
		- Sigmoid $\sigma(x) = \frac{1}{1+\exp(-x)}$
		- Minimize softplus
---
## CNN

---