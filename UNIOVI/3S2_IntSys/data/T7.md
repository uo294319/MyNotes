# Deep Learning & CNN
---
[Go Back](UNIOVI/3S2_IntSys/README.md)

---
## Deep Learning 
### Objective
- Given a dataset $D = {(x, y)  : x \leadsto y}$
- Design the non-linear function $f : R^m \to R^n, f(x, \theta) = f_\theta(x)$
- Find $\theta$ so that minimises the mean of the loss function
	- $\theta \leftarrow \frac{1}{|D|}\sum_{(x, y)\in D}loss(y, f_\theta(x))$
### Non linearity
- Activation functions
	- ReLu. $g(z) = \max(0, z)$ (most used nowadays)
	- Hyperbolic tangent. (-1 and 1)
	- Sigmoid. (0 and 1) (used for classification problems)
- Several nested layers:
	- Hidden layer 1. $h_1 = g(W_1x+b_1)$
	- Hidden layer 2. $h_2 = g(W_2h_1+b_2)$
	- Output layer. $g(x) =  g(W_2\times g(W_1x+b_1)+b_2)$
- Optimisation needs derivatives. Gradient Descent.
- Universal Approximator Theorem
	- One hidden layer is enough to represent an approximation of any function.
	- But choose wider networks because:
		- It avoids exponential # neurons needed.
		- Better generalisation and avoids overfitting.
### Types
- Feedforward or Multi-Layer Perceptrons (MLP) are fully connected.
- Convolutional Neural Networks (CNN). For processing data in grid-like topology.
- Recurrent Neural Networks (RNN). For processing sequential data. Extend MLP.
### Learning Tasks
- Regression.
	- $Pr(y|x, \theta) = N( y, f(x, \theta), \sigma^2)$.
	- Probability distribution involving $f$
	- Minimize MSE $\arg\min_\theta \sum_{D}(y - f_\theta(x, \theta))^2$
- Binary Classification
	- Search for sth proportional to $\exp(yf(x,\theta))$.
		- Not a probability distribution.
	- Options.
		- Softmax. 
			- Multiclass Classification. Not a probability distribution.
			- $Pr(y|x,\theta) = \frac{\exp(yf(x, \theta))}{1 + f(x, \theta)}$
		- Sigmoid
			- Particularisation of softmax for binary classification (0 and 1).
			- It is a probability distribution.
			- $\sigma(x) = \frac{1}{1+\exp(-x)}$ 
			- $Pr(y|x,\theta) = \sigma((2y-1)f(x,\theta))$
	- In both we minimize $-log\quad Pr(y|x, \theta)$
### Final Remarks
- It uses back-propagation to avoid recomputing repeated sub-expressions.
- **Regularization**. Modifications to reduce its generalisation error but not training one.
	- Norm Penalties.
		- Add an extra term to the loss function that penalizes large weights
		- Prevents over-fitting.
	- Early Stopping
		- Terminate the training once the validation set performance is better.
	- Dropout.
		- Create subDisable some non-output neurons during training forming subnets.
		- 
---
## CNN

---